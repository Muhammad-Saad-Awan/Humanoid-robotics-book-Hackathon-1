"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[25],{8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>s});var i=o(6540);const t={},a=i.createContext(t);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:n},e.children)}},8639:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4-vla/introduction","title":"Module 4: Vision-Language-Action (VLA) Overview","description":"The convergence of Generative AI and Robotics. Build a Cognitive Pipeline for humanoid robots.","source":"@site/docs/module-4-vla/introduction.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/introduction","permalink":"/Humanoid-robotics-book/docs/module-4-vla/introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/irza16/Humanoid-robotics-book/tree/main/docs/module-4-vla/introduction.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Module 4: Vision-Language-Action (VLA) Overview","description":"The convergence of Generative AI and Robotics. Build a Cognitive Pipeline for humanoid robots.","keywords":["vla","humanoid","robotics","generative ai","llm","ros2","capstone"]},"sidebar":"tutorialSidebar","previous":{"title":"Module 3 Capstone: End-to-End Perception Pipeline","permalink":"/Humanoid-robotics-book/docs/module-3-isaac/capstone-perception-pipeline"},"next":{"title":"Week 11: Humanoid Manipulation & Kinematics","permalink":"/Humanoid-robotics-book/docs/module-4-vla/week11-manipulation"}}');var t=o(4848),a=o(8453);const r={sidebar_position:1,title:"Module 4: Vision-Language-Action (VLA) Overview",description:"The convergence of Generative AI and Robotics. Build a Cognitive Pipeline for humanoid robots.",keywords:["vla","humanoid","robotics","generative ai","llm","ros2","capstone"]},s="Module 4: Vision-Language-Action (VLA) & Capstone",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Weekly Breakdown",id:"weekly-breakdown",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla--capstone",children:"Module 4: Vision-Language-Action (VLA) & Capstone"})}),"\n",(0,t.jsx)(n.p,{children:'Module 4 represents the pinnacle of our journey: the convergence of Generative AI and Robotics. In this module, you will learn to build a "Cognitive Pipeline" that allows a humanoid robot to understand natural language voice commands, plan complex tasks using Large Language Models (LLMs), and execute them via ROS 2 actions. The module culminates in the final Capstone Project: a fully integrated Autonomous Humanoid capable of hearing, thinking, seeing, and acting.'}),"\n",(0,t.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"Upon completing this module, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integrate OpenAI Whisper for accurate voice-to-text transcription."}),"\n",(0,t.jsx)(n.li,{children:'Engineer prompts for LLMs to act as robotic "Reasoning Agents."'}),"\n",(0,t.jsx)(n.li,{children:"Translate natural language (unstructured) into ROS 2 Actions (structured JSON)."}),"\n",(0,t.jsx)(n.li,{children:"Implement Vision-Language-Action (VLA) loops where vision modifies the plan."}),"\n",(0,t.jsx)(n.li,{children:"Coordinate kinematic motions (grasping/walking) based on AI decisions."}),"\n",(0,t.jsx)(n.li,{children:'Deploy and demonstrate the full "Sense-Think-Act" loop in a capstone demo.'}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision-Language-Action (VLA)"}),": Systems that ground language in physical perception."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech-to-Text (STT) & Text-to-Speech (TTS)"}),": Integration for natural human-robot communication."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cognitive Planning"}),": Using an LLM to break a high-level goal ('Clean room') into sequential steps."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Function Calling"}),": Technique to force LLMs to output structured data (JSON) enabling code execution."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Grounding"}),": Linking linguistic concepts (e.g., 'the red cup') to physical coordinates."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Behavior Orchestration"}),": Managing the state machine between listening, planning, and moving."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"weekly-breakdown",children:"Weekly Breakdown"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Week 11: Humanoid Manipulation & Kinematics"}),": Focus on bipedal locomotion basics, Inverse Kinematics (IK) for arm manipulation, grasping strategies, and MoveIt 2 integration."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Week 12: The Cognitive Brain (LLMs)"}),": Dive into speech recognition with OpenAI Whisper, prompt engineering for robots, connecting LLMs to ROS 2 via Python, and JSON output parsing with safety guardrails."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Week 13: Capstone Integration"}),": Integrate Voice, Vision (from Module 3), and Action (ROS 2). Learn to handle failures, replan, optimize latency, and prepare for the final project demonstration."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This module will provide you with the knowledge and skills to build truly intelligent and interactive humanoid robots. Get ready to bring your robot to life!"})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);