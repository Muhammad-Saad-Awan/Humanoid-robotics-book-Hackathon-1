"use strict";(globalThis.webpackChunkhumanoid_robotics_book=globalThis.webpackChunkhumanoid_robotics_book||[]).push([[106],{333:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vla/week12-cognitive-brain","title":"Week 12: The Cognitive Brain (LLMs)","description":"Explore speech recognition with OpenAI Whisper, prompt engineering for robots, connecting LLMs to ROS 2, and JSON output parsing with safety guardrails.","source":"@site/docs/module-4-vla/week12-cognitive-brain.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/week12-cognitive-brain","permalink":"/Humanoid-robotics-book/docs/module-4-vla/week12-cognitive-brain","draft":false,"unlisted":false,"editUrl":"https://github.com/irza16/Humanoid-robotics-book/tree/main/docs/module-4-vla/week12-cognitive-brain.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Week 12: The Cognitive Brain (LLMs)","description":"Explore speech recognition with OpenAI Whisper, prompt engineering for robots, connecting LLMs to ROS 2, and JSON output parsing with safety guardrails.","keywords":["week12","cognitive brain","llm","whisper","stt","prompt engineering","ros2","function calling","json","safety"]},"sidebar":"tutorialSidebar","previous":{"title":"Week 11: Humanoid Manipulation & Kinematics","permalink":"/Humanoid-robotics-book/docs/module-4-vla/week11-manipulation"},"next":{"title":"Week 13: Capstone Integration","permalink":"/Humanoid-robotics-book/docs/module-4-vla/week13-capstone"}}');var t=i(4848),r=i(8453);const s={sidebar_position:3,title:"Week 12: The Cognitive Brain (LLMs)",description:"Explore speech recognition with OpenAI Whisper, prompt engineering for robots, connecting LLMs to ROS 2, and JSON output parsing with safety guardrails.",keywords:["week12","cognitive brain","llm","whisper","stt","prompt engineering","ros2","function calling","json","safety"]},a="Week 12: The Cognitive Brain (LLMs)",l={},c=[{value:"Topics Covered",id:"topics-covered",level:2},{value:"1. Speech Recognition with OpenAI Whisper",id:"1-speech-recognition-with-openai-whisper",level:3},{value:"2. Prompt Engineering for Robots (Chain of Thought)",id:"2-prompt-engineering-for-robots-chain-of-thought",level:3},{value:"3. Connecting LLMs to ROS 2 via Python",id:"3-connecting-llms-to-ros-2-via-python",level:3},{value:"4. JSON Output Parsing and Safety Guardrails",id:"4-json-output-parsing-and-safety-guardrails",level:3},{value:"Practical Application",id:"practical-application",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"week-12-the-cognitive-brain-llms",children:"Week 12: The Cognitive Brain (LLMs)"})}),"\n",(0,t.jsx)(n.p,{children:"This week, we dive into the cognitive core of our humanoid robot: the integration of Large Language Models (LLMs) to enable advanced reasoning and natural language understanding. We will specifically focus on using OpenAI Whisper for speech-to-text, engineering prompts for robotic applications, seamlessly connecting LLMs to ROS 2, and ensuring safe and structured output through JSON parsing."}),"\n",(0,t.jsx)(n.h2,{id:"topics-covered",children:"Topics Covered"}),"\n",(0,t.jsx)(n.h3,{id:"1-speech-recognition-with-openai-whisper",children:"1. Speech Recognition with OpenAI Whisper"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Introduction to Speech-to-Text (STT)"}),": How spoken language is converted into text."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"OpenAI Whisper"}),": Overview of the model, its capabilities, and how to use it for high-accuracy transcription."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integration with Robotics"}),": Capturing audio from a robot's microphone and feeding it to Whisper."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Interface for STT"}),": Designing a ROS 2 node to manage audio input and publish transcribed text messages."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-prompt-engineering-for-robots-chain-of-thought",children:"2. Prompt Engineering for Robots (Chain of Thought)"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLMs as Reasoning Agents"}),": Leveraging LLMs for cognitive tasks like planning, problem-solving, and decision-making."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Effective Prompt Design"}),": Crafting prompts that guide the LLM to generate actionable plans relevant to a robot's physical actions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chain of Thought Prompting"}),": Techniques to enable LLMs to show their reasoning process, making their outputs more interpretable and reliable for robotic control."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Context Management"}),": Providing the LLM with relevant information about the robot's state and environment."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-connecting-llms-to-ros-2-via-python",children:"3. Connecting LLMs to ROS 2 via Python"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 - LLM Communication Architecture"}),": Designing the data flow between ROS 2 topics/services and an LLM API."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Python Client for LLMs"}),": Using ",(0,t.jsx)(n.code,{children:"openai"})," or ",(0,t.jsx)(n.code,{children:"anthropic"})," client libraries to interact with LLM APIs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Node for LLM Integration"}),": Developing a ROS 2 node that subscribes to text commands, calls the LLM, and publishes the resulting plan or actions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Asynchronous Processing"}),": Handling LLM API calls without blocking the robot's real-time control loop."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"4-json-output-parsing-and-safety-guardrails",children:"4. JSON Output Parsing and Safety Guardrails"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Function Calling"}),": A method to compel LLMs to output structured JSON that can be directly parsed into executable robot actions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"JSON Schema Validation"}),": Ensuring the LLM's output conforms to predefined robotic action schemas."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety Guardrails"}),": Implementing checks and balances to prevent the LLM from generating unsafe, invalid, or ambiguous actions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error Handling and Replanning"}),": Strategies for detecting and recovering from LLM output errors, potentially triggering a replanning phase."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"practical-application",children:"Practical Application"}),"\n",(0,t.jsx)(n.p,{children:"This week involves significant coding to build the cognitive pipeline. You will develop a ROS 2 node that integrates Whisper and an LLM, allowing the robot to take voice commands, process them cognitively, and output a structured action plan. Emphasis will be placed on robust error handling and safety."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var o=i(6540);const t={},r=o.createContext(t);function s(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);